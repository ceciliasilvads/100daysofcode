# **100 days of code - Python**

**Dia 1-7:** Fundamentos de Python
- [X] Dia 1-2: Instale o Python e um ambiente de desenvolvimento (como o Anaconda ou Jupyter Notebook).
- [X] Dia 1-2: Variáveis, tipos de dados e operadores,Estruturas condicionais (if, else, elif) e Loops (for e while), Listas, tuplas, conjuntos e dicionários em Python.
- [X] Dia 3-4: Funções
- [X] Dia 5: Trabalhe com módulos e bibliotecas padrão.
- [X] Dia 6-7: Aplicar conhecimentos da semana.

**Dia 8-14:** Introdução à programação orientada a objetos (POO).
- [X] Dia 8: Paradigmas de Programação
- [X] Dia 9-10: Classes, objetos e métodos.
- [ ] Dia 11-12: Herança e polimorfismo.
- [ ] Dia 13-14: Crie um pequeno projeto orientado a objetos.

**Dia 15-20:**: Manipulação de Arquivos e Exceções
- [ ] Dia 15-20: Trabalhe com arquivos (leitura e escrita).
- [ ] Dia 15-20: Manipule exceções e erros em Python.
- [ ] Dia 15-20: Desenvolva um programa que lida com arquivos e exceções.

**Dia 21-30:** Bancos de Dados e SQL em Python
- [ ] Dia 21: Introdução a bancos de dados relacionais.
- [ ] Dia 22-23: SQL básico para consulta de dados.
- [ ] Dia 24-25: Interagindo com bancos de dados SQL em Python (usando SQLAlchemy).
- [ ] Dia 26-30: Simples ETL (Extração, Transformação e Carga) com Python e bancos de dados.

**Dia 31-40:** Processamento de Dados com Pandas e NumPy
- [ ] Dia 31: Manipulação avançada de dados com Pandas (agregação, join).
- [ ] Dia 32-34: Introdução a NumPy para computação científica em Python.
- [ ] Dia 35-37: Trabalhar com datas e horas em Python.
- [ ] Dia 38-40: Projeto de análise de dados avançada com Pandas e NumPy.
      
**Dia 41-50:** Introdução ao Spark
- [ ] Dia 41: Configurar o ambiente Spark em Python (usando PySpark).
- [ ] Dia 42-44: Spark RDDs (Resilient Distributed Datasets) e transformações básicas.
- [ ] Dia 45-47: Spark DataFrames e SQL para análise de dados em larga escala.
- [ ] Dia 48-50: Projeto de processamento de dados em larga escala com Spark.

**Dia 51-60:** Streaming de Dados com Spark
- [ ] Dia 51: Introdução ao streaming de dados com Spark.
- [ ] Dia 52-55: Processamento de streaming em tempo real com Spark Streaming.
- [ ] Dia 56-60: Projeto de processamento de streaming de dados em tempo real.

**Dia 61-70:** Armazenamento de Dados e Integração com Bancos NoSQL
- [ ] Dia 61: Armazenamento de dados em armazenamentos em nuvem (por exemplo, AWS S3).
- [ ] Dia 62-65: Introdução a bancos de dados NoSQL (por exemplo, MongoDB) com Python.
- [ ] Dia 66-70: Integração de dados entre SQL e NoSQL em um projeto.

**Dia 71-80:** Orquestração de Fluxo de Dados com Apache Airflow
- [ ] Dia 71: Introdução ao Apache Airflow para orquestração de tarefas.
- [ ] Dia 72-75: Definir e agendar fluxos de trabalho com Apache Airflow.
- [ ] Dia 76-80: Projeto de orquestração de ETL com Apache Airflow.

**Dia 81-90:** Machine Learning para Engenharia de Dados em Python
- [ ] Dia 81: Introdução ao Machine Learning com Python.
- [ ] Dia 82-85: Pré-processamento de dados para ML com Pandas e Scikit-Learn.
- [ ] Dia 86-90: Implementação de modelos de Machine Learning para análise de dados.

**Dia 90-100:** Projeto Final de Engenharia de Dados em Python
- [ ] Dia 90-95: Escolha um projeto final que envolva coleta, processamento e análise de dados em Python.
- [ ] Dia 95-100: Desenvolva e finalize seu projeto, documentando-o bem e apresentando seus resultados.
